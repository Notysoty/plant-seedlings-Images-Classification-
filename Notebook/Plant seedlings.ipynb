{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da70f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224cc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500d7171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
    "              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
    "NUM_CATEGORIES = len(CATEGORIES)\n",
    "NUM_CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99249df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images count:\n",
      "Black-grass 309 images\n",
      "Charlock 452 images\n",
      "Cleavers 335 images\n",
      "Common Chickweed 713 images\n",
      "Common wheat 253 images\n",
      "Fat Hen 538 images\n",
      "Loose Silky-bent 762 images\n",
      "Maize 257 images\n",
      "Scentless Mayweed 607 images\n",
      "Shepherds Purse 274 images\n",
      "Small-flowered Cranesbill 576 images\n",
      "Sugar beet 463 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Total images count:\")\n",
    "for category in CATEGORIES:\n",
    "    print('{} {} images'.format(category, len(os.listdir(os.path.join(\"../NonsegmentedV2\", category)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e9216",
   "metadata": {},
   "source": [
    "%pip install split_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1161f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 5539 files [01:50, 50.16 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('../NonsegmentedV2/', output=\"../output\", seed=1337, ratio=(.8, 0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e80e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"../output/train\"\n",
    "NUM_TRAIN_EXAMPLES = 0\n",
    "for _, _, files in os.walk(train_folder):\n",
    "    NUM_TRAIN_EXAMPLES += len([fn for fn in files if fn.endswith(\".png\")])\n",
    "test_folder = \"../output/test\"\n",
    "NUM_TEST_EXAMPLES = 0\n",
    "for _, _, files in os.walk(test_folder):\n",
    "    NUM_TEST_EXAMPLES += len([fn for fn in files if fn.endswith(\".png\")])\n",
    "val_folder = \"../output/val\"\n",
    "NUM_VAL_EXAMPLES = 0\n",
    "for _, _, files in os.walk(val_folder):\n",
    "    NUM_VAL_EXAMPLES += len([fn for fn in files if fn.endswith(\".png\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3256c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainging images count:\n",
      "Black-grass 247 images\n",
      "Charlock 361 images\n",
      "Cleavers 268 images\n",
      "Common Chickweed 570 images\n",
      "Common wheat 202 images\n",
      "Fat Hen 430 images\n",
      "Loose Silky-bent 609 images\n",
      "Maize 205 images\n",
      "Scentless Mayweed 485 images\n",
      "Shepherds Purse 219 images\n",
      "Small-flowered Cranesbill 460 images\n",
      "Sugar beet 370 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainging images count:\")\n",
    "for category in CATEGORIES:\n",
    "    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_folder, category)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497c7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0945c7",
   "metadata": {},
   "source": [
    "fig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\n",
    "i = 0\n",
    "for category_id, category in enumerate(CATEGORIES):\n",
    "    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n",
    "        ax = grid[i]\n",
    "        img = read_img(filepath, (image_size[0], image_size[1]))\n",
    "        ax.imshow(img / 255.)\n",
    "        ax.axis('off')\n",
    "        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1:\n",
    "            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9e8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess():\n",
    "    # Preprocessing\n",
    "    TRAINING_DIR = \"../output/train\"\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255., \n",
    "                                    rotation_range=40, \n",
    "                                    width_shift_range=0.2, \n",
    "                                    height_shift_range=0.2, \n",
    "                                    shear_range=0.2, zoom_range=0.2, \n",
    "                                    horizontal_flip=True)\n",
    "    # TRAIN GENERATOR.\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "                        TRAINING_DIR,\n",
    "                        target_size =(150, 150),\n",
    "                        batch_size = 32,\n",
    "                        class_mode = 'categorical')\n",
    "\n",
    "    return train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7541e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess(DIR):\n",
    "    # Preprocessing\n",
    "    # VALIDATION GENERATOR.\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "                        DIR,\n",
    "                        target_size =(150, 150),\n",
    "                        batch_size = 10,\n",
    "                        class_mode = 'categorical')\n",
    "    return validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9512cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_model():\n",
    "    train_ds = train_preprocess()\n",
    "    val_ds = test_preprocess(\"../output/val\")\n",
    "    test_ds = test_preprocess(\"../output/test\")\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=image_size),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(12, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=30,\n",
    "        steps_per_epoch=NUM_TRAIN_EXAMPLES // 32,\n",
    "        validation_data=val_ds,\n",
    "        validation_steps=NUM_VAL_EXAMPLES//10\n",
    "    )\n",
    "    model.evaluate(test_ds, steps=NUM_TEST_EXAMPLES//10)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16d169fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4426 images belonging to 12 classes.\n",
      "Found 548 images belonging to 12 classes.\n",
      "Found 565 images belonging to 12 classes.\n",
      "Epoch 1/30\n",
      "138/138 [==============================] - 199s 1s/step - loss: 2.1133 - accuracy: 0.2685 - val_loss: 1.6347 - val_accuracy: 0.4648\n",
      "Epoch 2/30\n",
      "138/138 [==============================] - 114s 825ms/step - loss: 1.6018 - accuracy: 0.4320 - val_loss: 1.5051 - val_accuracy: 0.4333\n",
      "Epoch 3/30\n",
      "138/138 [==============================] - 116s 839ms/step - loss: 1.4102 - accuracy: 0.5000 - val_loss: 1.1770 - val_accuracy: 0.6185\n",
      "Epoch 4/30\n",
      "138/138 [==============================] - 116s 842ms/step - loss: 1.2573 - accuracy: 0.5555 - val_loss: 1.0694 - val_accuracy: 0.6574\n",
      "Epoch 5/30\n",
      "138/138 [==============================] - 119s 862ms/step - loss: 1.2181 - accuracy: 0.5792 - val_loss: 0.9978 - val_accuracy: 0.6556\n",
      "Epoch 6/30\n",
      "138/138 [==============================] - 117s 847ms/step - loss: 1.1457 - accuracy: 0.6083 - val_loss: 1.0130 - val_accuracy: 0.6704\n",
      "Epoch 7/30\n",
      "138/138 [==============================] - 114s 826ms/step - loss: 1.0816 - accuracy: 0.6224 - val_loss: 0.8908 - val_accuracy: 0.6796\n",
      "Epoch 8/30\n",
      "138/138 [==============================] - 114s 821ms/step - loss: 1.0301 - accuracy: 0.6561 - val_loss: 0.9679 - val_accuracy: 0.6407\n",
      "Epoch 9/30\n",
      "138/138 [==============================] - 109s 788ms/step - loss: 0.9936 - accuracy: 0.6691 - val_loss: 0.7686 - val_accuracy: 0.7519\n",
      "Epoch 10/30\n",
      "138/138 [==============================] - 105s 755ms/step - loss: 0.9106 - accuracy: 0.6866 - val_loss: 0.7498 - val_accuracy: 0.7537\n",
      "Epoch 11/30\n",
      "138/138 [==============================] - 111s 804ms/step - loss: 0.8767 - accuracy: 0.7073 - val_loss: 0.6558 - val_accuracy: 0.8000\n",
      "Epoch 12/30\n",
      "138/138 [==============================] - 113s 814ms/step - loss: 0.7765 - accuracy: 0.7381 - val_loss: 0.5661 - val_accuracy: 0.8037\n",
      "Epoch 13/30\n",
      "138/138 [==============================] - 107s 771ms/step - loss: 0.7575 - accuracy: 0.7451 - val_loss: 0.5306 - val_accuracy: 0.8167\n",
      "Epoch 14/30\n",
      "138/138 [==============================] - 110s 799ms/step - loss: 0.7091 - accuracy: 0.7606 - val_loss: 0.5938 - val_accuracy: 0.7852\n",
      "Epoch 15/30\n",
      "138/138 [==============================] - 111s 801ms/step - loss: 0.6487 - accuracy: 0.7786 - val_loss: 0.4388 - val_accuracy: 0.8426\n",
      "Epoch 16/30\n",
      "138/138 [==============================] - 88s 638ms/step - loss: 0.6118 - accuracy: 0.7845 - val_loss: 0.4667 - val_accuracy: 0.8407\n",
      "Epoch 17/30\n",
      "138/138 [==============================] - 82s 595ms/step - loss: 0.6116 - accuracy: 0.7952 - val_loss: 0.4504 - val_accuracy: 0.8481\n",
      "Epoch 18/30\n",
      "138/138 [==============================] - 83s 597ms/step - loss: 0.5505 - accuracy: 0.8081 - val_loss: 0.3819 - val_accuracy: 0.8759\n",
      "Epoch 19/30\n",
      "138/138 [==============================] - 84s 605ms/step - loss: 0.5240 - accuracy: 0.8252 - val_loss: 0.3411 - val_accuracy: 0.8889\n",
      "Epoch 20/30\n",
      "138/138 [==============================] - 82s 597ms/step - loss: 0.5059 - accuracy: 0.8207 - val_loss: 0.3775 - val_accuracy: 0.8685\n",
      "Epoch 21/30\n",
      "138/138 [==============================] - 83s 596ms/step - loss: 0.4905 - accuracy: 0.8289 - val_loss: 0.3984 - val_accuracy: 0.8741\n",
      "Epoch 22/30\n",
      "138/138 [==============================] - 55s 398ms/step - loss: 0.4672 - accuracy: 0.8318 - val_loss: 0.5832 - val_accuracy: 0.7944\n",
      "Epoch 23/30\n",
      "138/138 [==============================] - 51s 371ms/step - loss: 0.4801 - accuracy: 0.8300 - val_loss: 0.3501 - val_accuracy: 0.8574\n",
      "Epoch 24/30\n",
      "138/138 [==============================] - 51s 364ms/step - loss: 0.4565 - accuracy: 0.8384 - val_loss: 0.3020 - val_accuracy: 0.8889\n",
      "Epoch 25/30\n",
      "138/138 [==============================] - 49s 353ms/step - loss: 0.4609 - accuracy: 0.8373 - val_loss: 0.2914 - val_accuracy: 0.8889\n",
      "Epoch 26/30\n",
      "138/138 [==============================] - 50s 363ms/step - loss: 0.4308 - accuracy: 0.8432 - val_loss: 0.3015 - val_accuracy: 0.8870\n",
      "Epoch 27/30\n",
      "138/138 [==============================] - 50s 364ms/step - loss: 0.4080 - accuracy: 0.8571 - val_loss: 0.3220 - val_accuracy: 0.8870\n",
      "Epoch 28/30\n",
      "138/138 [==============================] - 51s 367ms/step - loss: 0.4092 - accuracy: 0.8512 - val_loss: 0.3346 - val_accuracy: 0.8704\n",
      "Epoch 29/30\n",
      "138/138 [==============================] - 50s 363ms/step - loss: 0.3925 - accuracy: 0.8641 - val_loss: 0.2797 - val_accuracy: 0.8963\n",
      "Epoch 30/30\n",
      "138/138 [==============================] - 51s 371ms/step - loss: 0.3919 - accuracy: 0.8566 - val_loss: 0.2634 - val_accuracy: 0.9074\n",
      "56/56 [==============================] - 6s 114ms/step - loss: 0.2777 - accuracy: 0.8982\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = solution_model()\n",
    "    model.save('../Model/Plant_Seedling.h5')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daffaa7917ec11d9792e89e7c13ba1f31895757b00e88637b8f27ad14cb9a283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
