{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da70f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224cc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500d7171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
    "              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
    "NUM_CATEGORIES = len(CATEGORIES)\n",
    "NUM_CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99249df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images count:\n",
      "Black-grass 309 images\n",
      "Charlock 452 images\n",
      "Cleavers 335 images\n",
      "Common Chickweed 713 images\n",
      "Common wheat 253 images\n",
      "Fat Hen 538 images\n",
      "Loose Silky-bent 762 images\n",
      "Maize 257 images\n",
      "Scentless Mayweed 607 images\n",
      "Shepherds Purse 274 images\n",
      "Small-flowered Cranesbill 576 images\n",
      "Sugar beet 463 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Total images count:\")\n",
    "for category in CATEGORIES:\n",
    "    print('{} {} images'.format(category, len(os.listdir(os.path.join(\"../NonsegmentedV2\", category)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea6e9216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting split_folders\n",
      "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: split-folders\n",
      "Successfully installed split-folders-0.5.1\n"
     ]
    }
   ],
   "source": [
    "%pip install split_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1161f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 5539 files [01:42, 53.81 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('../NonsegmentedV2/', output=\"../output\", seed=1337, ratio=(.8, 0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e80e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"../output/train\"\n",
    "NUM_TRAIN_EXAMPLES = 0\n",
    "for _, _, files in os.walk(train_folder):\n",
    "    NUM_TRAIN_EXAMPLES += len([fn for fn in files if fn.endswith(\".png\")])\n",
    "test_folder = \"../output/test\"\n",
    "NUM_TEST_EXAMPLES = 0\n",
    "for _, _, files in os.walk(test_folder):\n",
    "    NUM_TEST_EXAMPLES += len([fn for fn in files if fn.endswith(\".png\")])\n",
    "val_folder = \"../output/val\"\n",
    "NUM_VAL_EXAMPLES = 0\n",
    "for _, _, files in os.walk(val_folder):\n",
    "    NUM_VAL_EXAMPLES += len([fn for fn in files if fn.endswith(\".png\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3256c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainging images count:\n",
      "Black-grass 247 images\n",
      "Charlock 361 images\n",
      "Cleavers 268 images\n",
      "Common Chickweed 570 images\n",
      "Common wheat 202 images\n",
      "Fat Hen 430 images\n",
      "Loose Silky-bent 609 images\n",
      "Maize 205 images\n",
      "Scentless Mayweed 485 images\n",
      "Shepherds Purse 219 images\n",
      "Small-flowered Cranesbill 460 images\n",
      "Sugar beet 370 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainging images count:\")\n",
    "for category in CATEGORIES:\n",
    "    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_folder, category)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "497c7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0945c7",
   "metadata": {},
   "source": [
    "fig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\n",
    "i = 0\n",
    "for category_id, category in enumerate(CATEGORIES):\n",
    "    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n",
    "        ax = grid[i]\n",
    "        img = read_img(filepath, (image_size[0], image_size[1]))\n",
    "        ax.imshow(img / 255.)\n",
    "        ax.axis('off')\n",
    "        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1:\n",
    "            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec9e8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess():\n",
    "    # Preprocessing\n",
    "    TRAINING_DIR = \"../output/train\"\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255., \n",
    "                                    rotation_range=40, \n",
    "                                    width_shift_range=0.2, \n",
    "                                    height_shift_range=0.2, \n",
    "                                    shear_range=0.2, zoom_range=0.2, \n",
    "                                    horizontal_flip=True)\n",
    "    # TRAIN GENERATOR.\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "                        TRAINING_DIR,\n",
    "                        target_size =(150, 150),\n",
    "                        batch_size = 32,\n",
    "                        class_mode = 'categorical')\n",
    "\n",
    "    return train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7541e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess(DIR):\n",
    "    # Preprocessing\n",
    "    # VALIDATION GENERATOR.\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "                        DIR,\n",
    "                        target_size =(150, 150),\n",
    "                        batch_size = 10,\n",
    "                        class_mode = 'categorical')\n",
    "    return validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9512cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_model():\n",
    "    train_ds = train_preprocess()\n",
    "    val_ds = test_preprocess(\"../output/val\")\n",
    "    test_ds = test_preprocess(\"../output/test\")\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=image_size),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(12, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=30,\n",
    "        steps_per_epoch=NUM_TRAIN_EXAMPLES // 32,\n",
    "        validation_data=val_ds,\n",
    "        validation_steps=NUM_VAL_EXAMPLES//10\n",
    "    )\n",
    "    model.evaluate(test_ds, steps=NUM_TEST_EXAMPLES//10)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d169fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4426 images belonging to 12 classes.\n",
      "Found 548 images belonging to 12 classes.\n",
      "Found 565 images belonging to 12 classes.\n",
      "Epoch 1/30\n",
      "138/138 [==============================] - 199s 1s/step - loss: 2.1605 - accuracy: 0.2440 - val_loss: 1.7678 - val_accuracy: 0.3667\n",
      "Epoch 2/30\n",
      "138/138 [==============================] - 105s 761ms/step - loss: 1.7813 - accuracy: 0.3605 - val_loss: 1.5740 - val_accuracy: 0.4648\n",
      "Epoch 3/30\n",
      "138/138 [==============================] - 107s 773ms/step - loss: 1.5995 - accuracy: 0.4306 - val_loss: 1.3349 - val_accuracy: 0.5500\n",
      "Epoch 4/30\n",
      "138/138 [==============================] - 110s 800ms/step - loss: 1.4265 - accuracy: 0.4904 - val_loss: 1.2119 - val_accuracy: 0.5796\n",
      "Epoch 5/30\n",
      "138/138 [==============================] - 106s 766ms/step - loss: 1.3052 - accuracy: 0.5494 - val_loss: 1.2616 - val_accuracy: 0.5593\n",
      "Epoch 6/30\n",
      "138/138 [==============================] - 111s 801ms/step - loss: 1.2358 - accuracy: 0.5706 - val_loss: 1.0279 - val_accuracy: 0.6352\n",
      "Epoch 7/30\n",
      "138/138 [==============================] - 110s 794ms/step - loss: 1.1207 - accuracy: 0.6067 - val_loss: 0.8714 - val_accuracy: 0.6981\n",
      "Epoch 8/30\n",
      "138/138 [==============================] - 111s 804ms/step - loss: 1.0630 - accuracy: 0.6402 - val_loss: 0.8488 - val_accuracy: 0.6907\n",
      "Epoch 9/30\n",
      "138/138 [==============================] - 109s 791ms/step - loss: 0.9724 - accuracy: 0.6711 - val_loss: 0.6830 - val_accuracy: 0.7593\n",
      "Epoch 10/30\n",
      "138/138 [==============================] - 109s 787ms/step - loss: 0.8915 - accuracy: 0.7037 - val_loss: 0.6169 - val_accuracy: 0.7852\n",
      "Epoch 11/30\n",
      "138/138 [==============================] - 106s 765ms/step - loss: 0.8444 - accuracy: 0.7144 - val_loss: 0.6930 - val_accuracy: 0.7685\n",
      "Epoch 12/30\n",
      "138/138 [==============================] - 109s 788ms/step - loss: 0.7823 - accuracy: 0.7335 - val_loss: 0.5728 - val_accuracy: 0.7981\n",
      "Epoch 13/30\n",
      "138/138 [==============================] - 112s 813ms/step - loss: 0.7438 - accuracy: 0.7467 - val_loss: 0.5243 - val_accuracy: 0.8222\n",
      "Epoch 14/30\n",
      "138/138 [==============================] - 109s 791ms/step - loss: 0.7016 - accuracy: 0.7540 - val_loss: 0.5294 - val_accuracy: 0.8333\n",
      "Epoch 15/30\n",
      "138/138 [==============================] - 111s 803ms/step - loss: 0.6751 - accuracy: 0.7649 - val_loss: 0.5607 - val_accuracy: 0.8000\n",
      "Epoch 16/30\n",
      "138/138 [==============================] - 108s 783ms/step - loss: 0.6477 - accuracy: 0.7792 - val_loss: 0.4787 - val_accuracy: 0.8352\n",
      "Epoch 17/30\n",
      "138/138 [==============================] - 100s 724ms/step - loss: 0.6139 - accuracy: 0.7824 - val_loss: 0.5044 - val_accuracy: 0.8278\n",
      "Epoch 18/30\n",
      "138/138 [==============================] - 103s 743ms/step - loss: 0.6030 - accuracy: 0.7906 - val_loss: 0.4677 - val_accuracy: 0.8315\n",
      "Epoch 19/30\n",
      "138/138 [==============================] - 106s 763ms/step - loss: 0.6165 - accuracy: 0.7886 - val_loss: 0.4559 - val_accuracy: 0.8463\n",
      "Epoch 20/30\n",
      "138/138 [==============================] - 101s 731ms/step - loss: 0.5564 - accuracy: 0.8088 - val_loss: 0.4579 - val_accuracy: 0.8389\n",
      "Epoch 21/30\n",
      "138/138 [==============================] - 106s 768ms/step - loss: 0.5526 - accuracy: 0.8072 - val_loss: 0.4103 - val_accuracy: 0.8574\n",
      "Epoch 22/30\n",
      "138/138 [==============================] - 106s 766ms/step - loss: 0.5301 - accuracy: 0.8120 - val_loss: 0.3700 - val_accuracy: 0.8722\n",
      "Epoch 23/30\n",
      "138/138 [==============================] - 113s 815ms/step - loss: 0.5092 - accuracy: 0.8232 - val_loss: 0.3778 - val_accuracy: 0.8833\n",
      "Epoch 24/30\n",
      "138/138 [==============================] - 109s 786ms/step - loss: 0.5013 - accuracy: 0.8198 - val_loss: 0.3885 - val_accuracy: 0.8648\n",
      "Epoch 25/30\n",
      "  2/138 [..............................] - ETA: 1:08 - loss: 0.6736 - accuracy: 0.7188"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = solution_model()\n",
    "    model.save('../Model/Plant_Seedling.h5')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daffaa7917ec11d9792e89e7c13ba1f31895757b00e88637b8f27ad14cb9a283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
